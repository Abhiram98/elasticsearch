[[discovery-troubleshooting]]
== Troubleshooting discovery

In most cases, the <<modules-discovery,discovery and election>> process completes quickly, and the
master node remains elected for a long period of time.

If your cluster doesn't have a stable master, many of its features won't work
correctly and {es} will report errors to clients and in its logs, such as `master_not_discovered_exception`, 
HTTP `503 Service Unavailable`, or `ClusterBlockException` from `SERVICE_UNAVAILABLE`. You must fix
the master node's instability before addressing these other issues. It will not
be possible to solve any other issues while there is no elected master node or
the elected master node is unstable.

If your cluster has a stable master but some nodes can't discover or join it,
these nodes will report errors to clients and in their logs. You must address
the obstacles preventing these nodes from joining the cluster before addressing
other issues. It will not be possible to solve any other issues reported by
these nodes while they are unable to join the cluster.

If the cluster has no elected master node for more than a few seconds, the
master is unstable, or some nodes are unable to discover or join a stable
master, then {es} will record information in its logs explaining why. If the
problems persist for more than a few minutes, {es} will record additional
information in its logs. To properly troubleshoot discovery and election
problems, collect and analyse logs covering at least five minutes from all
nodes.

The following sections describe some common discovery and election problems.

[discrete]
[[discovery-no-master]]
=== No master is elected

When a node wins the master election, it logs a message containing
`elected-as-master` and all nodes log a message containing
`master node changed` identifying the new elected master node.

If there is no elected master node and no node can win an election, all
nodes will repeatedly log messages about the problem using a logger called
`org.elasticsearch.cluster.coordination.ClusterFormationFailureHelper`. By
default, this happens every 10 seconds.

Master elections only involve master-eligible nodes, so focus your attention on
the master-eligible nodes in this situation. These nodes' logs will indicate
the requirements for a master election, such as the discovery of a certain set
of nodes. The <<health-api>> API on these nodes will also provide useful
information about the situation.

If the logs or the health report indicate that {es} can't discover enough nodes
to form a quorum, you must address the reasons preventing {es} from discovering
the missing nodes. The missing nodes are needed to reconstruct the cluster
metadata. Without the cluster metadata, the data in your cluster is
meaningless. The cluster metadata is stored on a subset of the master-eligible
nodes in the cluster. If a quorum can't be discovered, the missing nodes were
the ones holding the cluster metadata.

Ensure there are enough nodes running to form a quorum and that every node can
communicate with every other node over the network. {es} will report additional
details about network connectivity if the election problems persist for more
than a few minutes. If you can't start enough nodes to form a quorum, start a
new cluster and restore data from a recent snapshot. Refer to
<<modules-discovery-quorums>> for more information.

If the logs or the health report indicate that {es} _has_ discovered a possible
quorum of nodes, the typical reason that the cluster can't elect a master is
that one of the other nodes can't discover a quorum. Inspect the logs on the
other master-eligible nodes and ensure that they have all discovered enough
nodes to form a quorum.

If the logs suggest that discovery or master elections are failing due to
timeouts or network-related issues then narrow down the problem as follows.

include::network-timeouts.asciidoc[tag=troubleshooting-network-timeouts-gc-vm]

include::network-timeouts.asciidoc[tag=troubleshooting-network-timeouts-packet-capture-elections]

include::network-timeouts.asciidoc[tag=troubleshooting-network-timeouts-threads]

[discrete]
[[discovery-master-unstable]]
=== Master is elected but unstable

When a node wins the master election, it logs a message containing
`elected-as-master`. If this happens repeatedly, the elected master node is
unstable. In this situation, focus on the logs from the master-eligible nodes
to understand why the election winner stops being the master and triggers
another election. If the logs suggest that the master is unstable due to
timeouts or network-related issues then narrow down the problem as follows.

include::network-timeouts.asciidoc[tag=troubleshooting-network-timeouts-gc-vm]

include::network-timeouts.asciidoc[tag=troubleshooting-network-timeouts-packet-capture-elections]

include::network-timeouts.asciidoc[tag=troubleshooting-network-timeouts-threads]

[discrete]
[[discovery-cannot-join-master]]
=== Node cannot discover or join stable master

If there is a stable elected master but a node can't discover or join its
cluster, it will repeatedly log messages about the problem using the
`ClusterFormationFailureHelper` logger. The <<health-api>> API on the affected
node will also provide useful information about the situation. Other log
messages on the affected node and the elected master may provide additional
information about the problem. If the logs suggest that the node cannot
discover or join the cluster due to timeouts or network-related issues then
narrow down the problem as follows.

include::network-timeouts.asciidoc[tag=troubleshooting-network-timeouts-gc-vm]

include::network-timeouts.asciidoc[tag=troubleshooting-network-timeouts-packet-capture-elections]

include::network-timeouts.asciidoc[tag=troubleshooting-network-timeouts-threads]

[discrete]
[[discovery-node-leaves]]
=== Node joins cluster and leaves again

If a node joins the cluster but {es} determines it to be faulty then it will be
removed from the cluster again. See <<cluster-fault-detection-troubleshooting>>
for more information.

[discrete]
[[discovery-master-split-brain]]
=== Multiple masters are elected

Infrequently, {es} can enter a {wikipedia}/Split-brain_(computing)[split-brain] 
state, where nodes within a cluster disagree on which node is elected 
as master node. This is a non-viable state for the cluster, and can lead to 
inconsistencies and data loss. 

To prevent state from occurring, we recommend the following practices:

<<modules-discovery-voting,Avoid even 
master-eligible node counts>> by creating an odd number of master-eligible nodes or adding a voting-only <<high-availability-cluster-design-two-nodes, tiebreaker>>

* Remove the <<initial_master_nodes,`cluster.initial_master_nodes` 
setting>> after <<modules-discovery-bootstrap-cluster,bootstrapping a cluster>>

To mitigate the impact of split-brain, we recommend having a <<high-availability,highly 
available cluster>> and <<snapshot-restore,snapshots to restore from>> as needed. 

{ess} and {ece} deployments can automatically detect split-brain. If the state arises, these deployment types report `The requested resource is currently split` when polling the 
<<rest-apis,{es} API>>. 

For other deployment types, you can't detect split-brain 
from within a cluster. Instead, you have to check for split-brain per node, using one of the following signals: 

* If, when polling the <<cluster-state,Cluster state API>> with the `local=true` flag, polled nodes report differing master nodes:
+
[source,console]
--------------------------------------------------
GET /_cluster/state/master_node?local=true
--------------------------------------------------
+ 
Where for example the `cluster_name` would overlap but the `master_node` would 
disagree across the two checked node's response bodies
+ 
[source,json]
--------------------------------------------------
{
  "cluster_name": "0fds584fes1es09s88se6c01j4290eed",
  "cluster_uuid": "4spssZBssnshgVqWPiDzCA",
  "master_node": "Zg1jXgyMQa2GavDr7MmIDA"
}
--------------------------------------------------

* If a node's logs contain messages about cluster UUID disagreements, such as:

** `trying to join a different cluster with UUID [YYYYY]`:
+
[source,txt]
--------------------------------------------------
org.elasticsearch.cluster.coordination.CoordinationStateRejectedException: 
This node previously joined a cluster with UUID [XXXXX] and is now 
trying to join a different cluster with UUID [YYYYY]. 
This is forbidden and usually indicates an incorrect discovery or 
cluster bootstrapping configuration. 
Note that the cluster UUID persists across restarts and can only be 
changed by deleting the contents of the node's data path [/app/data] 
which will also remove any data held by this node.
--------------------------------------------------

** `join validation on cluster state with a different cluster uuid [XXXXX]`
+
[source,txt]
--------------------------------------------------
org.elasticsearch.cluster.coordination.CoordinationStateRejectedException: 
join validation on cluster state with a different cluster uuid XXXXX 
than local cluster uuid YYYYY, rejecting
--------------------------------------------------

To resolve split-brain, do the following:

. Choose which elected master to continue running the cluster. Usually 
this will be the one recognized by the most nodes or containing the most desired data. 

. Stop any nodes pointing to the other master or masters. While each node is stopped, remove the <<initial_master_nodes,`cluster.initial_master_nodes` setting>>, if present.

. Attempt to restart the nodes. Start-up logs will show if the node is willing 
and able to join the existing master, or if it refuses to leave previous cluster UUID. 

** If the nodes are able to join to the existing elected master node's cluster, 
you've resolved the current split-brain issue. 

** If the nodes are unwilling to leave the previous cluster UUID, then your cluster is at risk of losing data and you need to take additional steps to resolve the cluster state. Consider <<node-tool-unsafe-bootstrap,unsafe 
bootstrapping>> the node. 
